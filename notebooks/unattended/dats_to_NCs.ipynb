{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "973c3511",
   "metadata": {},
   "source": [
    "# Combine dat files from unattended ApRES surveys on Thwaites glacier into netcdfs for archiving\n",
    "### J. Kingslake July 2nd 2025\n",
    "The code below collates the dat filed collected in unattneded ApRES surveys on Thwaites Glacier between 2022 and 2024 by the GHOST team, part of the International Thwaites Glacier Collaboration. The data were collected by Elizabeth Case, Columbia University, then Utrecht University, Andrew Hoffman, Columbia University, then Rice University, and Ole Zeisling, Alfred Wegner Institute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b53ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xapres as xa\n",
    "import xarray as xr\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99dc6c3a",
   "metadata": {},
   "source": [
    "Define where the dat files are and where we will write the netcdfs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96913230",
   "metadata": {},
   "outputs": [],
   "source": [
    "sites = [\"ApRES_LTG\", \"ApRES_Lake1\", \"ApRES_Lake2\", \"ApRES_Takahe1_204\", \"ApRES_Takahe2_203\"]\n",
    "source = \"/Users/jkingslake/Documents/data/thwaites_apres/original/continuous/\"\n",
    "archive_location ='/Users/jkingslake/Documents/data/thwaites_apres/archiving/unattended'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21946fdd",
   "metadata": {},
   "source": [
    "Define a function for changing the format of the attributes in the xarray to work nicely when written to netcdf. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b467bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_constants_attr(ds):\n",
    "    for c in ds.attrs['constants']:\n",
    "        ds.attrs[c] = ds.attrs['constants'][c]\n",
    "    del ds.attrs['constants']\n",
    "    return ds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e153cb3",
   "metadata": {},
   "source": [
    "Loop through the loacations and\n",
    "1. find all the dat files, subset them, and sort them by filename, so that they are in time order. \n",
    "2. Split the list of dat files into blocks with a size that results in reasonably sized netcdfs. \n",
    "3. Define netcdf paths and make directoies for them\n",
    "4. Delete previously created netcdfs \n",
    "5. loop through the blocks of files and load all the dat files into an xarray using the package xapres\n",
    "6. add the stacked chirps\n",
    "7. tidy up the units and attributes to work well with netcdfs\n",
    "8. write the netcdfs\n",
    "(the last three steps are performed for each block of files) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc68ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tg_1 = xa.load.from_dats()\n",
    "for site in sites:\n",
    "    # 1. find all the dat files, subset them, and sort them by filename, so that they are in time order. \n",
    "    dat_directory = os.path.join(source, site, \"dat_files\")\n",
    "    filepaths = tg_1.list_files(directory=dat_directory)\n",
    "    print(f\"there are {len(filepaths)} files from {site}\")\n",
    "    one_dat = tg_1.load_all(directory = dat_directory, file_numbers_to_process=[7], computeProfiles = False, disable_progress_bar=True)\n",
    "    print(f\"total size of dataset without profiles computed = {one_dat.nbytes/1e9 * len(filepaths):.2f} GB in memory\")\n",
    "\n",
    "    if site == 'ApRES_LTG': \n",
    "        start_dat_file_number = 4  # the first 4 use the same attenuator settings, but they are not regularly spaces in time with the other data\n",
    "    elif site == 'ApRES_Lake1':\n",
    "        start_dat_file_number = 3  # the first 3 use different attenuator settings. THis is the first one which is regularly spaced in time with the other data\n",
    "    elif site == 'ApRES_Lake2':\n",
    "        start_dat_file_number = 4  # This is the first one which is regularly spaced in time with the other data\n",
    "    elif site == 'ApRES_Takahe1_204':\n",
    "        start_dat_file_number = 0\n",
    "    elif site == 'ApRES_Takahe2_203':\n",
    "        start_dat_file_number = 3 # something is up with the first 3 files some seem to have no data and one is the wrong size. \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown site {site}\")\n",
    "\n",
    "    dat_size = os.path.getsize(filepaths[start_dat_file_number+5])\n",
    "    print(f\"dat file size = {dat_size/1e6:.2f} MB on disk\")\n",
    "\n",
    "    f = sorted(filepaths)[start_dat_file_number:]\n",
    "\n",
    "    # 2. Split the list of dat files into blocks with a size that results in reasonably sized netcdfs. \n",
    "    N = int(17*104e6/dat_size)  # number of files to process in each block\n",
    "    file_lists = [f[i:i + N] for i in range(0, len(f), N)]\n",
    "    print(f'created {len(file_lists)} file lists from {len(f)} files')\n",
    "    print(f'list sizes: {[len(x) for x in file_lists]}')\n",
    "\n",
    "    # 3. Define netcdf paths and make directoies for them\n",
    "    nc_locations = [f'{archive_location}/{site}/netcdf/part_{i}.nc' for i in range(len(file_lists))]\n",
    "    os.makedirs(f\"{archive_location}/{site}/netcdf\", exist_ok=True) \n",
    "    \n",
    "    # 4. Delete previously created netcdfs \n",
    "    for nc in nc_locations:\n",
    "        if os.path.exists(nc):\n",
    "            print(f\"deleting existing file {nc}\")\n",
    "            os.remove(nc)\n",
    "\n",
    "    # 5. loop through the blocks of files and load all the dat files into an xarray using the package xapres\n",
    "    for i, file_list in enumerate(file_lists):\n",
    "        print(f\"Processing {len(file_list)} files\")\n",
    "        ds = tg_1.load_all(directory = dat_directory, \n",
    "                        computeProfiles = False,\n",
    "                        file_names_to_process=file_list,\n",
    "                        disable_progress_bar=False)\n",
    "        \n",
    "        # 6. add the stacked chirps\n",
    "        ds['chirp_stacked'] = ds.chirp.mean(dim='chirp_num', keep_attrs=True)\n",
    "        \n",
    "        # 7. tidy up the units and attributes to work well with netcdfs\n",
    "        ds.chirp_stacked.attrs['long_name'] = 'stacked de-ramped chirp'\n",
    "        ds[\"chirp_time\"].attrs[\"units\"] = \"unscaled seconds\"\n",
    "        ds = remove_constants_attr(ds)\n",
    "        \n",
    "        # 8. write the netcdfs\n",
    "        ds.to_netcdf(nc_locations[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39867566",
   "metadata": {},
   "source": [
    "## Reload the data\n",
    "To combine the multiple netcdfs into one xarray, use xarray's `open_mfdataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e51bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_r = xr.open_mfdataset(f\"{archive_location}/{sites[3]}/netcdf/*\")\n",
    "ds_r"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "apres_archiving",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
